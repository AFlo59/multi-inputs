{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (5.28.3)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.3)\n",
      "Requirement already satisfied: optree in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: namex in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: transformers in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (4.46.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from transformers) (0.26.1)\n",
      "Requirement already satisfied: requests in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: filelock in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: pandas in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/utilisateur/Bureau/week2/multi-inputs/venv/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV chargé avec succès.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './Flipkart/flipkart_com-ecommerce_sample_1050.csv'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"CSV chargé avec succès.\")\n",
    "else:\n",
    "    print(f\"Erreur : Le fichier n'existe pas à l'emplacement {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajoutez le chemin des images et préparez les labels de catégories\n",
    "data['image_path'] = data['image'].apply(lambda x: f'./Flipkart/Images/{x}')\n",
    "data['category'] = data['product_category_tree'].apply(lambda x: x.split(\">>\")[0].replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', '').strip())\n",
    "categories = data['category'].unique().tolist()\n",
    "category_to_index = {cat: idx for idx, cat in enumerate(categories)}\n",
    "data['category_id'] = data['category'].map(category_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_text = data['description'].fillna(\"\")\n",
    "X_image = data['image_path']\n",
    "y = data['category_id']\n",
    "X_train_text, X_test_text, X_train_image, X_test_image, y_train, y_test = train_test_split(X_text, X_image, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1730199589.199206  245099 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-10-29 11:59:49.304679: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2024-10-29 11:59:49.440085: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2024-10-29 11:59:49.464439: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2024-10-29 11:59:50.494582: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
    "import tensorflow as tf\n",
    "import tf_keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Charger le modèle DistilBERT sans tête de classification\n",
    "base_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Ajout de la couche de classification\n",
    "class TextClassificationModel(Model):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.base_model(inputs)\n",
    "        # Prendre la moyenne de last_hidden_state\n",
    "        pooled_output = tf.reduce_mean(outputs.last_hidden_state, axis=1)\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "# Initialiser le modèle de classification de texte\n",
    "text_model = TextClassificationModel(base_model, num_classes=len(categories))\n",
    "\n",
    "# Compiler le modèle\n",
    "text_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \n",
    "                   loss=\"sparse_categorical_crossentropy\", \n",
    "                   metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Importation de numpy\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Prétraitement des images\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "# Chargement des images\n",
    "X_train_image_processed = np.array([load_and_preprocess_image(img_path) for img_path in X_train_image])\n",
    "X_test_image_processed = np.array([load_and_preprocess_image(img_path) for img_path in X_test_image])\n",
    "\n",
    "# Modèle de classification d'image\n",
    "base_model = EfficientNetB0(include_top=False, input_shape=(224, 224, 3), weights=\"imagenet\")\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "output = Dense(len(categories), activation=\"softmax\")(x)\n",
    "image_model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "image_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entraînement du modèle\n",
    "image_model.fit(X_train_image_processed, y_train, batch_size=16, epochs=3, validation_data=(X_test_image_processed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFDistilBertModel\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "# Entrée pour le texte\n",
    "text_input = Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "text_mask = Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "text_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "text_features = text_model(text_input, attention_mask=text_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "# Entrée pour l'image\n",
    "image_input = Input(shape=(224, 224, 3), name=\"image_input\")\n",
    "base_image_model = EfficientNetB0(include_top=False, input_shape=(224, 224, 3), weights=\"imagenet\")\n",
    "image_features = GlobalAveragePooling2D()(base_image_model(image_input))\n",
    "\n",
    "# Fusion des deux sorties\n",
    "combined_features = Concatenate()([text_features, image_features])\n",
    "output = Dense(len(categories), activation=\"softmax\")(combined_features)\n",
    "\n",
    "# Modèle multi-input\n",
    "multi_input_model = Model(inputs=[text_input, text_mask, image_input], outputs=output)\n",
    "\n",
    "# Compilation du modèle\n",
    "multi_input_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entraînement du modèle\n",
    "multi_input_model.fit(\n",
    "    [train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], X_train_image_processed],\n",
    "    y_train,\n",
    "    batch_size=16,\n",
    "    epochs=3,\n",
    "    validation_data=([test_encodings[\"input_ids\"], test_encodings[\"attention_mask\"], X_test_image_processed], y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les données d'entrée\n",
    "def predict_text_category(description):\n",
    "    inputs = tokenizer(description, return_tensors=\"tf\", padding=\"max_length\", truncation=True)\n",
    "    outputs = text_model(inputs)\n",
    "    prediction = tf.argmax(outputs, axis=1).numpy()[0]\n",
    "    return categories[prediction]\n",
    "\n",
    "# Exemple de prédiction\n",
    "description_test = \"A powerful all-in-one computer with last generation components\"\n",
    "print(\"Predicted Category:\", predict_text_category(description_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_image_from_url(url):\n",
    "    # Télécharger l'image depuis l'URL\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Ouvrir l'image avec PIL et la convertir au format attendu par le modèle\n",
    "    image = Image.open(response.raw).convert(\"RGB\")\n",
    "    image = image.resize((224, 224))  # Redimensionner l'image\n",
    "    image = np.array(image) / 255.0   # Normaliser les pixels entre 0 et 1\n",
    "    return image\n",
    "\n",
    "def predict_image_category_from_url(url):\n",
    "    # Charger et prétraiter l'image depuis l'URL\n",
    "    image = load_and_preprocess_image_from_url(url)\n",
    "    image = tf.expand_dims(image, axis=0)  # Ajouter une dimension pour le batch\n",
    "\n",
    "    # Prédire la catégorie\n",
    "    prediction = tf.argmax(image_model.predict(image), axis=1).numpy()[0]\n",
    "    return categories[prediction]\n",
    "\n",
    "# URL de l'image\n",
    "image_url = \"https://cdn.pixabay.com/photo/2020/10/21/18/07/laptop-5673901_1280.jpg\"\n",
    "print(\"Predicted Category:\", predict_image_category_from_url(image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def load_and_preprocess_image_from_url(url):\n",
    "    # Télécharger l'image depuis l'URL\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Ouvrir l'image avec PIL et la convertir au format attendu par le modèle\n",
    "    image = Image.open(response.raw).convert(\"RGB\")\n",
    "    image = image.resize((224, 224))  # Redimensionner l'image\n",
    "    image = np.array(image) / 255.0   # Normaliser les pixels entre 0 et 1\n",
    "    return image\n",
    "\n",
    "def predict_multi_input(description, image_url):\n",
    "    # Prétraitement de la description\n",
    "    text_inputs = tokenizer(description, return_tensors=\"tf\", padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Charger et prétraiter l'image depuis l'URL\n",
    "    image = load_and_preprocess_image_from_url(image_url)\n",
    "    image = tf.expand_dims(image, axis=0)  # Ajouter une dimension pour le batch\n",
    "\n",
    "    # Prédire la catégorie\n",
    "    prediction = tf.argmax(\n",
    "        multi_input_model.predict(\n",
    "            {\"input_ids\": text_inputs[\"input_ids\"], \"attention_mask\": text_inputs[\"attention_mask\"], \"image_input\": image}\n",
    "        ), axis=1\n",
    "    ).numpy()[0]\n",
    "    return categories[prediction]\n",
    "\n",
    "# Exemple d'URL de l'image et description\n",
    "description_test = \"TV support table wood style rétro\"\n",
    "image_url = \"https://cdn1.hellin.fr/26695-zoom_default/meuble-tv-retro-en-bois-2-niches-2-portes-l180-mallet.jpg\"\n",
    "print(\"Predicted Category:\", predict_multi_input(description_test, image_url))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des poids du modèle de texte\n",
    "text_model.save_weights('/content/text_model_weights')\n",
    "\n",
    "# Sauvegarde des poids du modèle d'image\n",
    "image_model.save_weights('/content/image_model_weights')\n",
    "\n",
    "# Sauvegarde des poids du modèle multi-input\n",
    "multi_input_model.save_weights('/content/multi_input_model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Recréer la structure du modèle de texte\n",
    "class TextClassificationModel(Model):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.base_model(inputs)\n",
    "        pooled_output = tf.reduce_mean(outputs.last_hidden_state, axis=1)\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "# Instancier et charger les poids\n",
    "base_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "text_model = TextClassificationModel(base_model, num_classes=len(categories))\n",
    "text_model.load_weights('path/to/text_model_weights')\n",
    "\n",
    "# Procédez de même pour le modèle d'image et le modèle multi-input\n",
    "# Exemple pour le modèle d'image\n",
    "image_model = ...  # recréez la structure du modèle d'image\n",
    "image_model.load_weights('path/to/image_model_weights')\n",
    "\n",
    "# Exemple pour le modèle multi-input\n",
    "multi_input_model = ...  # recréez la structure du modèle multi-input\n",
    "multi_input_model.load_weights('path/to/multi_input_model_weights')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
